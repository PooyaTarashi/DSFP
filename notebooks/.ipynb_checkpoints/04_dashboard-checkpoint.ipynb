{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12a9b656",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdfs import InsecureClient\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import losses, metrics\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "85b86b5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accessTime': 0,\n",
       " 'blockSize': 0,\n",
       " 'childrenNum': 1,\n",
       " 'fileId': 16385,\n",
       " 'group': 'supergroup',\n",
       " 'length': 0,\n",
       " 'modificationTime': 1770282148988,\n",
       " 'owner': 'root',\n",
       " 'pathSuffix': '',\n",
       " 'permission': '755',\n",
       " 'replication': 0,\n",
       " 'snapshotEnabled': True,\n",
       " 'storagePolicy': 0,\n",
       " 'type': 'DIRECTORY'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Establish connection to HDFS\n",
    "connection_url = \"http://localhost:9870\"\n",
    "hdfs_client = InsecureClient(connection_url, user='root')\n",
    "hdfs_client.status('/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "742b4842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "model_path = \"/home/pooya/Desktop/DSFP/models/weather_lstm_model.h5\"\n",
    "\n",
    "custom_objects = {\n",
    "    'mse': losses.MeanSquaredError(),\n",
    "    'mae': metrics.MeanAbsoluteError()\n",
    "}\n",
    "\n",
    "model = keras.models.load_model(\n",
    "    model_path,\n",
    "    custom_objects=custom_objects,\n",
    "    compile=False  # Don't compile during load\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=losses.MeanSquaredError(),\n",
    "    metrics=[metrics.MeanAbsoluteError()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a4c7f99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Albuquerque',\n",
       " 'Atlanta',\n",
       " 'Beersheba',\n",
       " 'Boston',\n",
       " 'Charlotte',\n",
       " 'Chicago',\n",
       " 'Dallas',\n",
       " 'Denver',\n",
       " 'Detroit',\n",
       " 'Eilat',\n",
       " 'Haifa',\n",
       " 'Houston',\n",
       " 'Indianapolis',\n",
       " 'Jacksonville',\n",
       " 'Jerusalem',\n",
       " 'Kansas City',\n",
       " 'Las Vegas',\n",
       " 'Los Angeles',\n",
       " 'Miami',\n",
       " 'Minneapolis',\n",
       " 'Montreal',\n",
       " 'Nahariyya',\n",
       " 'Nashville',\n",
       " 'New York',\n",
       " 'Philadelphia',\n",
       " 'Phoenix',\n",
       " 'Pittsburgh',\n",
       " 'Portland',\n",
       " 'Saint Louis',\n",
       " 'San Antonio',\n",
       " 'San Diego',\n",
       " 'San Francisco',\n",
       " 'Seattle',\n",
       " 'Tel Aviv District',\n",
       " 'Toronto',\n",
       " 'Vancouver']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_path = '/weather/raw'\n",
    "\n",
    "locations = []\n",
    "for item in hdfs_client.list(raw_path):\n",
    "    if item.startswith('location='):\n",
    "        location = item.replace('location=', '')\n",
    "        locations.append(location)\n",
    "\n",
    "locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6d5994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_recent_data(location, hours=48):\n",
    "    # Path to location partition\n",
    "    partition_path = f'/weather/raw/location={location}'\n",
    "    \n",
    "    # Read parquet files\n",
    "    parquet_files = []\n",
    "    for file in hdfs_client.list(partition_path):\n",
    "        if file.endswith('.parquet'):\n",
    "            file_path = f'{partition_path}/{file}'\n",
    "            parquet_files.append(file_path)\n",
    "\n",
    "\n",
    "    import pyarrow as pa\n",
    "    import pyarrow.parquet as pq\n",
    "\n",
    "    fs = pa.connect(host='localhost', port=9005, user='root')\n",
    "\n",
    "    dfs = []\n",
    "    for file_path in parquet_files:\n",
    "        table = pq.read_table(file_path, filesystem=fs)\n",
    "        df = table.to_pandas()\n",
    "        dfs.append(df)\n",
    "\n",
    "    # Problem:\n",
    "    # with hdfs_client.read(parquet_files[0]) as f:\n",
    "    #     print(f)\n",
    "\n",
    "    return parquet_files    # For debugging purposes -> works fine\n",
    "    \n",
    "    # # Read all parquet files\n",
    "    dfs = []\n",
    "    for file_path in parquet_files:\n",
    "        with hdfs_client.read(file_path) as reader:\n",
    "            df = pd.read_parquet(reader)\n",
    "            dfs.append(df)\n",
    "\n",
    "    # return dfs    # For debugging purposes -> problematic\n",
    "    \n",
    "    # # Combine all dataframes\n",
    "    # combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # # Convert timestamp to datetime\n",
    "    # combined_df['timestamp'] = pd.to_datetime(combined_df['timestamp'])\n",
    "    \n",
    "    # # Sort by timestamp\n",
    "    # combined_df = combined_df.sort_values('timestamp')\n",
    "    \n",
    "    # # Filter to recent hours\n",
    "    # cutoff_time = datetime.now() - timedelta(hours=hours)\n",
    "    # recent_df = combined_df[combined_df['timestamp'] >= cutoff_time]\n",
    "    \n",
    "    # logger.info(f\"Fetched {len(recent_df)} records for {location}\")\n",
    "    # return recent_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6184f522",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pyarrow' has no attribute 'hdfs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfetch_recent_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAlbuquerque\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[32], line 16\u001b[0m, in \u001b[0;36mfetch_recent_data\u001b[0;34m(location, hours)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpa\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpq\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m fs \u001b[38;5;241m=\u001b[39m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhdfs\u001b[49m\u001b[38;5;241m.\u001b[39mconnect(host\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocalhost\u001b[39m\u001b[38;5;124m'\u001b[39m, port\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m9005\u001b[39m, user\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroot\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     18\u001b[0m dfs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m parquet_files:\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pyarrow' has no attribute 'hdfs'"
     ]
    }
   ],
   "source": [
    "fetch_recent_data('Albuquerque')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2cb74e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12a9b656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hdfs\n",
      "  Downloading hdfs-2.7.3.tar.gz (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m262.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting docopt (from hdfs)\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.7.0 in /opt/conda/lib/python3.11/site-packages (from hdfs) (2.31.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.11/site-packages (from hdfs) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.7.0->hdfs) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.7.0->hdfs) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.7.0->hdfs) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.7.0->hdfs) (2023.7.22)\n",
      "Building wheels for collected packages: hdfs, docopt\n",
      "  Building wheel for hdfs (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for hdfs: filename=hdfs-2.7.3-py3-none-any.whl size=34325 sha256=932b726aab982d5e9a24f1e79e5dde37f306f7020babd6665324ba7e432674b8\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/b9/1d/dc/eb0833be25464c359903d356c4204721c6a672c26ff164cdc3\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13705 sha256=3147df1b7b3788b977130dee8db66bb563ba66ea14d6978124223f0fa7c8a77c\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
      "Successfully built hdfs docopt\n",
      "Installing collected packages: docopt, hdfs\n",
      "Successfully installed docopt-0.6.2 hdfs-2.7.3\n"
     ]
    }
   ],
   "source": [
    "!pip install hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f9786d-b3b3-44d1-a4d7-007fbd15039c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Downloading absl_py-2.4.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Downloading flatbuffers-25.12.19-py2.py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.7.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting google_pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt_einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from tensorflow) (23.2)\n",
      "Collecting protobuf>=5.28.0 (from tensorflow)\n",
      "  Downloading protobuf-6.33.5-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (4.8.0)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Downloading wrapt-2.1.1-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (7.4 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.76.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.7 kB)\n",
      "Collecting tensorboard~=2.20.0 (from tensorflow)\n",
      "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.10.0 (from tensorflow)\n",
      "  Downloading keras-3.13.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting numpy>=1.26.0 (from tensorflow)\n",
      "  Downloading numpy-2.4.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting h5py>=3.11.0 (from tensorflow)\n",
      "  Downloading h5py-3.15.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting ml_dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.5.4-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
      "Collecting typing_extensions>=3.6.6 (from tensorflow)\n",
      "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting rich (from keras>=3.10.0->tensorflow)\n",
      "  Downloading rich-14.3.2-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.10.0->tensorflow)\n",
      "  Downloading namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.10.0->tensorflow)\n",
      "  Downloading optree-0.18.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (34 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2023.7.22)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading markdown-3.10.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.11/site-packages (from tensorboard~=2.20.0->tensorflow) (10.1.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading werkzeug-3.1.5-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in /opt/conda/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (2.1.3)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.10.0->tensorflow)\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich->keras>=3.10.0->tensorflow) (2.16.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading tensorflow-2.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (620.6 MB)\n",
      "\u001b[2K   \u001b[91m━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.1/620.6 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:59\u001b[0mm"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0467486-06e7-46dd-ac40-cb1f872bb780",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdfs import InsecureClient\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import losses, metrics\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "85b86b5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accessTime': 0,\n",
       " 'blockSize': 0,\n",
       " 'childrenNum': 1,\n",
       " 'fileId': 16385,\n",
       " 'group': 'supergroup',\n",
       " 'length': 0,\n",
       " 'modificationTime': 1770282148988,\n",
       " 'owner': 'root',\n",
       " 'pathSuffix': '',\n",
       " 'permission': '755',\n",
       " 'replication': 0,\n",
       " 'snapshotEnabled': True,\n",
       " 'storagePolicy': 0,\n",
       " 'type': 'DIRECTORY'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Establish connection to HDFS\n",
    "connection_url = \"http://localhost:9870\"\n",
    "hdfs_client = InsecureClient(connection_url, user='root')\n",
    "hdfs_client.status('/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "742b4842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "model_path = \"/home/pooya/Desktop/DSFP/models/weather_lstm_model.h5\"\n",
    "\n",
    "custom_objects = {\n",
    "    'mse': losses.MeanSquaredError(),\n",
    "    'mae': metrics.MeanAbsoluteError()\n",
    "}\n",
    "\n",
    "model = keras.models.load_model(\n",
    "    model_path,\n",
    "    custom_objects=custom_objects,\n",
    "    compile=False  # Don't compile during load\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=losses.MeanSquaredError(),\n",
    "    metrics=[metrics.MeanAbsoluteError()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a4c7f99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Albuquerque',\n",
       " 'Atlanta',\n",
       " 'Beersheba',\n",
       " 'Boston',\n",
       " 'Charlotte',\n",
       " 'Chicago',\n",
       " 'Dallas',\n",
       " 'Denver',\n",
       " 'Detroit',\n",
       " 'Eilat',\n",
       " 'Haifa',\n",
       " 'Houston',\n",
       " 'Indianapolis',\n",
       " 'Jacksonville',\n",
       " 'Jerusalem',\n",
       " 'Kansas City',\n",
       " 'Las Vegas',\n",
       " 'Los Angeles',\n",
       " 'Miami',\n",
       " 'Minneapolis',\n",
       " 'Montreal',\n",
       " 'Nahariyya',\n",
       " 'Nashville',\n",
       " 'New York',\n",
       " 'Philadelphia',\n",
       " 'Phoenix',\n",
       " 'Pittsburgh',\n",
       " 'Portland',\n",
       " 'Saint Louis',\n",
       " 'San Antonio',\n",
       " 'San Diego',\n",
       " 'San Francisco',\n",
       " 'Seattle',\n",
       " 'Tel Aviv District',\n",
       " 'Toronto',\n",
       " 'Vancouver']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_path = '/weather/raw'\n",
    "\n",
    "locations = []\n",
    "for item in hdfs_client.list(raw_path):\n",
    "    if item.startswith('location='):\n",
    "        location = item.replace('location=', '')\n",
    "        locations.append(location)\n",
    "\n",
    "locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6d5994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_recent_data(location, hours=48):\n",
    "    # Path to location partition\n",
    "    partition_path = f'/weather/raw/location={location}'\n",
    "    \n",
    "    # Read parquet files\n",
    "    parquet_files = []\n",
    "    for file in hdfs_client.list(partition_path):\n",
    "        if file.endswith('.parquet'):\n",
    "            file_path = f'{partition_path}/{file}'\n",
    "            parquet_files.append(file_path)\n",
    "\n",
    "\n",
    "    import pyarrow as pa\n",
    "    import pyarrow.parquet as pq\n",
    "\n",
    "    fs = pa.connect(host='localhost', port=9005, user='root')\n",
    "\n",
    "    dfs = []\n",
    "    for file_path in parquet_files:\n",
    "        table = pq.read_table(file_path, filesystem=fs)\n",
    "        df = table.to_pandas()\n",
    "        dfs.append(df)\n",
    "\n",
    "    # Problem:\n",
    "    # with hdfs_client.read(parquet_files[0]) as f:\n",
    "    #     print(f)\n",
    "\n",
    "    return parquet_files    # For debugging purposes -> works fine\n",
    "    \n",
    "    # # Read all parquet files\n",
    "    dfs = []\n",
    "    for file_path in parquet_files:\n",
    "        with hdfs_client.read(file_path) as reader:\n",
    "            df = pd.read_parquet(reader)\n",
    "            dfs.append(df)\n",
    "\n",
    "    # return dfs    # For debugging purposes -> problematic\n",
    "    \n",
    "    # # Combine all dataframes\n",
    "    # combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # # Convert timestamp to datetime\n",
    "    # combined_df['timestamp'] = pd.to_datetime(combined_df['timestamp'])\n",
    "    \n",
    "    # # Sort by timestamp\n",
    "    # combined_df = combined_df.sort_values('timestamp')\n",
    "    \n",
    "    # # Filter to recent hours\n",
    "    # cutoff_time = datetime.now() - timedelta(hours=hours)\n",
    "    # recent_df = combined_df[combined_df['timestamp'] >= cutoff_time]\n",
    "    \n",
    "    # logger.info(f\"Fetched {len(recent_df)} records for {location}\")\n",
    "    # return recent_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6184f522",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pyarrow' has no attribute 'hdfs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfetch_recent_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAlbuquerque\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[32], line 16\u001b[0m, in \u001b[0;36mfetch_recent_data\u001b[0;34m(location, hours)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpa\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpq\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m fs \u001b[38;5;241m=\u001b[39m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhdfs\u001b[49m\u001b[38;5;241m.\u001b[39mconnect(host\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocalhost\u001b[39m\u001b[38;5;124m'\u001b[39m, port\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m9005\u001b[39m, user\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroot\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     18\u001b[0m dfs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m parquet_files:\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pyarrow' has no attribute 'hdfs'"
     ]
    }
   ],
   "source": [
    "fetch_recent_data('Albuquerque')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2cb74e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

networks:
  hadoop-network:
    name: hadoop-network
    driver: bridge

services:
  # =========================
  # Zookeeper
  # =========================
  zookeeper:
    image: confluentinc/cp-zookeeper:7.2.1
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
    ports:
      - "2181:2181"
    networks:
      - hadoop-network

  # =========================
  # Kafka
  # =========================
  kafka:
    image: confluentinc/cp-kafka:7.2.1
    container_name: kafka
    hostname: kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092,DOCKER://kafka:29092
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,DOCKER://0.0.0.0:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,DOCKER:PLAINTEXT
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
    volumes:
      - kafka-data:/var/lib/kafka/data
    networks:
      - hadoop-network

  # =========================
  # Kafka Connect
  # =========================
  connect:
    image: confluentinc/cp-kafka-connect:7.2.1
    container_name: connect
    depends_on:
      - kafka
    ports:
      - "8083:8083"
    environment:
      CONNECT_BOOTSTRAP_SERVERS: "kafka:29092"
      CONNECT_REST_PORT: "8083"
      CONNECT_REST_ADVERTISED_HOST_NAME: "connect"
      CONNECT_GROUP_ID: "connect-cluster"
      CONNECT_CONFIG_STORAGE_TOPIC: "connect-configs"
      CONNECT_OFFSET_STORAGE_TOPIC: "connect-offsets"
      CONNECT_STATUS_STORAGE_TOPIC: "connect-status"
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_INTERNAL_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_INTERNAL_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_PLUGIN_PATH: "/usr/share/java,/etc/kafka-connect/jars"
    volumes:
      - ./local_folder_container:/tmp/input
    networks:
      - hadoop-network

  # =========================
  # Spark Master
  # =========================
  spark-master:
    image: registry.docker.ir/apache/spark:3.5.8-scala2.12-java11-python3-r-ubuntu
    container_name: spark-master
    command: >
      /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
      --host spark-master
      --port 7077
      --webui-port 8080
    ports:
      - "7077:7077"
      - "8080:8080"
    environment:
      - SPARK_LOCAL_IP=spark-master
    volumes:
      - ./spark-jobs:/opt/spark-apps
    networks:
      - hadoop-network

  # =========================
  # Spark Workers (3 Nodes)
  # =========================
  spark-worker-1:
    image: registry.docker.ir/apache/spark:3.5.8-scala2.12-java11-python3-r-ubuntu
    container_name: spark-worker-1
    depends_on:
      - spark-master
    command: >
      /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker
      spark://spark-master:7077
      --webui-port 8081
    ports:
      - "8081:8081"
    environment:
      - SPARK_LOCAL_IP=spark-worker-1
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
    networks:
      - hadoop-network

  spark-worker-2:
    image: registry.docker.ir/apache/spark:3.5.8-scala2.12-java11-python3-r-ubuntu
    container_name: spark-worker-2
    depends_on:
      - spark-master
    command: >
      /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker
      spark://spark-master:7077
      --webui-port 8082
    ports:
      - "8082:8082"
    environment:
      - SPARK_LOCAL_IP=spark-worker-2
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
    networks:
      - hadoop-network

  spark-worker-3:
    image: registry.docker.ir/apache/spark:3.5.8-scala2.12-java11-python3-r-ubuntu
    container_name: spark-worker-3
    depends_on:
      - spark-master
    command: >
      /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker
      spark://spark-master:7077
      --webui-port 8085
    ports:
      - "8085:8085"
    environment:
      - SPARK_LOCAL_IP=spark-worker-3
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
    networks:
      - hadoop-network

  # =========================
  # HDFS NameNode
  # =========================
  hdfs-namenode:
    image: bde2020/hadoop-namenode
    container_name: hdfs-namenode
    environment:
      - CLUSTER_NAME=weather-cluster
      - CORE_CONF_fs_defaultFS=hdfs://hdfs-namenode:9005
    ports:
      - "9870:9870"
      - "9005:9005"
    volumes:
      - namenode:/hadoop/dfs/name
    networks:
      - hadoop-network

  # =========================
  # HDFS DataNodes (3 Storage Nodes)
  # =========================
  hdfs-datanode-1:
    image: bde2020/hadoop-datanode
    container_name: hdfs-datanode-1
    depends_on:
      - hdfs-namenode
    environment:
      - SERVICE_PRECONDITION=hdfs-namenode:9870
      - CORE_CONF_fs_defaultFS=hdfs://hdfs-namenode:9005
    volumes:
      - datanode1:/hadoop/dfs/data
    networks:
      - hadoop-network

  hdfs-datanode-2:
    image: bde2020/hadoop-datanode
    container_name: hdfs-datanode-2
    depends_on:
      - hdfs-namenode
    environment:
      - SERVICE_PRECONDITION=hdfs-namenode:9870
      - CORE_CONF_fs_defaultFS=hdfs://hdfs-namenode:9005
    volumes:
      - datanode2:/hadoop/dfs/data
    networks:
      - hadoop-network

  hdfs-datanode-3:
    image: bde2020/hadoop-datanode
    container_name: hdfs-datanode-3
    depends_on:
      - hdfs-namenode
    environment:
      - SERVICE_PRECONDITION=hdfs-namenode:9870
      - CORE_CONF_fs_defaultFS=hdfs://hdfs-namenode:9005
    volumes:
      - datanode3:/hadoop/dfs/data
    networks:
      - hadoop-network


  # =========================
  # Weather Fetcher (Producer)
  # =========================
  weather-fetcher:
    build: ./weather-fetcher
    container_name: weather-fetcher
    depends_on:
      - kafka
    environment:
      - KAFKA_BROKER=kafka:29092
      - TOPIC=weather_raw
    networks:
      - hadoop-network

  # =========================
  # Jupyter Notebook
  # =========================
  jupyter-notebook:
    image: docker.arvancloud.ir/jupyter/all-spark-notebook:latest
    container_name: jupyter-notebook
    depends_on:
      - spark-master
      - hdfs-namenode
      - kafka
    ports:
      - "8888:8888"
      - "4040:4040"  # Spark UI port
    environment:
      - JUPYTER_ENABLE_LAB=yes
      - SPARK_MASTER=spark://spark-master:7077
      - HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop
      - HADOOP_HOME=/usr/local/hadoop
      - HDFS_NAMENODE=hdfs://hdfs-namenode:9005
      - KAFKA_BROKER=kafka:29092
      - PYSPARK_PYTHON=python3
    volumes:
      - ./notebooks:/home/jovyan/work  # Mount local notebooks directory
      - ./spark-jobs:/home/jovyan/spark-jobs  # Mount spark jobs
      - ./local_folder_container:/home/jovyan/data  # Mount your data directory
      - ./models:/home/jovyan/models
    command: >
      start-notebook.sh
      --NotebookApp.token='' 
      --NotebookApp.password=''
      --ip=0.0.0.0
      --port=8888
      --allow-root
    networks:
      - hadoop-network

  # =========================
  # Dashboard (CLI + Web UI)
  # =========================
  # dashboard:
  #   build: ./dashboard
  #   container_name: dashboard
  #   depends_on:
  #     - hdfs-namenode
  #     - kafka
  #   environment:
  #     - HDFS_URL=http://hdfs-namenode:9870
  #     - MODEL_PATH=/app/models/weather_lstm_model.h5
  #   ports:
  #     - "8000:8000"   # CLI access
  #     - "8501:8501"   # Streamlit web UI
  #   volumes:
  #     - ./models:/app/models  # Mount your trained model here
  #   networks:
  #     - hadoop-network
  #   # Choose one of these commands:
  #   # For CLI mode:
  #   # command: ["python", "dashboard_app.py"]
  #   # For Web UI mode (uncomment this and comment above):
  #   command: ["streamlit", "run", "dashboard_web.py", "--server.port=8501", "--server.address=0.0.0.0"]

volumes:
  namenode:
  datanode1:
  datanode2:
  datanode3:
  kafka-data:

networks:
  hadoop-network:
    name: hadoop-network
    driver: bridge

services:
  # =========================
  # Zookeeper
  # =========================
  zookeeper:
    image: confluentinc/cp-zookeeper:7.2.1
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
    ports:
      - "2181:2181"
    networks:
      - hadoop-network

  # =========================
  # Kafka
  # =========================
  kafka:
    image: confluentinc/cp-kafka:7.2.1
    container_name: kafka
    hostname: kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092,DOCKER://kafka:29092
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,DOCKER://0.0.0.0:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,DOCKER:PLAINTEXT
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
    volumes:
      - kafka-data:/var/lib/kafka/data
    networks:
      - hadoop-network

  # =========================
  # Kafka Connect
  # =========================
  connect:
    image: confluentinc/cp-kafka-connect:7.2.1
    container_name: connect
    depends_on:
      - kafka
    ports:
      - "8083:8083"
    environment:
      CONNECT_BOOTSTRAP_SERVERS: "kafka:29092"
      CONNECT_REST_PORT: "8083"
      CONNECT_REST_ADVERTISED_HOST_NAME: "connect"
      CONNECT_GROUP_ID: "connect-cluster"
      CONNECT_CONFIG_STORAGE_TOPIC: "connect-configs"
      CONNECT_OFFSET_STORAGE_TOPIC: "connect-offsets"
      CONNECT_STATUS_STORAGE_TOPIC: "connect-status"
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_INTERNAL_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_INTERNAL_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_PLUGIN_PATH: "/usr/share/java,/etc/kafka-connect/jars"
    volumes:
      - ./local_folder_container:/tmp/input
    networks:
      - hadoop-network

  # =========================
  # Spark Master
  # =========================
  spark-master:
    image: registry.docker.ir/apache/spark:3.5.8-scala2.12-java11-python3-r-ubuntu
    container_name: spark-master
    command: >
      /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
      --host spark-master
      --port 7077
      --webui-port 8080
    ports:
      - "7077:7077"
      - "8080:8080"
    environment:
      - SPARK_LOCAL_IP=spark-master
    volumes:
      - ./spark-jobs:/opt/spark-apps
    networks:
      - hadoop-network

  # =========================
  # Spark Workers (3 Nodes)
  # =========================
  spark-worker-1:
    image: registry.docker.ir/apache/spark:3.5.8-scala2.12-java11-python3-r-ubuntu
    container_name: spark-worker-1
    depends_on:
      - spark-master
    command: >
      /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker
      spark://spark-master:7077
      --webui-port 8081
    ports:
      - "8081:8081"
    environment:
      - SPARK_LOCAL_IP=spark-worker-1
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
    networks:
      - hadoop-network

  spark-worker-2:
    image: registry.docker.ir/apache/spark:3.5.8-scala2.12-java11-python3-r-ubuntu
    container_name: spark-worker-2
    depends_on:
      - spark-master
    command: >
      /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker
      spark://spark-master:7077
      --webui-port 8082
    ports:
      - "8082:8082"
    environment:
      - SPARK_LOCAL_IP=spark-worker-2
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
    networks:
      - hadoop-network

  spark-worker-3:
    image: registry.docker.ir/apache/spark:3.5.8-scala2.12-java11-python3-r-ubuntu
    container_name: spark-worker-3
    depends_on:
      - spark-master
    command: >
      /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker
      spark://spark-master:7077
      --webui-port 8085
    ports:
      - "8085:8085"
    environment:
      - SPARK_LOCAL_IP=spark-worker-3
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
    networks:
      - hadoop-network

  # =========================
  # HDFS NameNode
  # =========================
  hdfs-namenode:
    image: bde2020/hadoop-namenode
    container_name: hdfs-namenode
    environment:
      - CLUSTER_NAME=weather-cluster
      - CORE_CONF_fs_defaultFS=hdfs://hdfs-namenode:9000
    ports:
      - "9870:9870"
      - "9000:9000"
    volumes:
      - namenode:/hadoop/dfs/name
    networks:
      - hadoop-network

  # =========================
  # HDFS DataNodes (3 Storage Nodes)
  # =========================
  hdfs-datanode-1:
    image: bde2020/hadoop-datanode
    container_name: hdfs-datanode-1
    depends_on:
      - hdfs-namenode
    environment:
      - SERVICE_PRECONDITION=hdfs-namenode:9870
      - CORE_CONF_fs_defaultFS=hdfs://hdfs-namenode:9000
    volumes:
      - datanode1:/hadoop/dfs/data
    networks:
      - hadoop-network

  hdfs-datanode-2:
    image: bde2020/hadoop-datanode
    container_name: hdfs-datanode-2
    depends_on:
      - hdfs-namenode
    environment:
      - SERVICE_PRECONDITION=hdfs-namenode:9870
      - CORE_CONF_fs_defaultFS=hdfs://hdfs-namenode:9000
    volumes:
      - datanode2:/hadoop/dfs/data
    networks:
      - hadoop-network

  hdfs-datanode-3:
    image: bde2020/hadoop-datanode
    container_name: hdfs-datanode-3
    depends_on:
      - hdfs-namenode
    environment:
      - SERVICE_PRECONDITION=hdfs-namenode:9870
      - CORE_CONF_fs_defaultFS=hdfs://hdfs-namenode:9000
    volumes:
      - datanode3:/hadoop/dfs/data
    networks:
      - hadoop-network

  # =========================
  # HDFS Initiator
  # =========================
  hdfs-init:
    image: registry.docker.ir/bde2020/hadoop-base:2.0.0-hadoop3.2.1-java8
    container_name: hdfs-init
    depends_on:
      - hdfs-namenode
      - hdfs-datanode-1
      - hdfs-datanode-2
      - hdfs-datanode-3
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://hdfs-namenode:9000
    command: >
      bash -c "
      hdfs dfs -mkdir -p /weather/raw /weather/aggregated /weather/checkpoints/raw /weather/checkpoints/aggregated
      hdfs dfs -chmod -R 777 /weather
      echo 'Directory structure created:'
      hdfs dfs -ls -R /
      "
    networks:
      - hadoop-network

  # =========================
  # Weather Fetcher (Producer)
  # =========================
  weather-fetcher:
    build: ./weather-fetcher
    container_name: weather-fetcher
    depends_on:
      - kafka
    environment:
      - KAFKA_BROKER=kafka:29092
      - TOPIC=weather_raw
    networks:
      - hadoop-network

  # =========================
  # Dashboard (Placeholder)
  # =========================
  dashboard:
    image: docker.arvancloud.ir/python:3.11
    container_name: dashboard
    command: ["sleep", "infinity"]
    ports:
      - "8000:8000"
    networks:
      - hadoop-network

volumes:
  namenode:
  datanode1:
  datanode2:
  datanode3:
  kafka-data: